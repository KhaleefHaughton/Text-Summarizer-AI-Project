# Text-Summarizer-AI-Project
Using LLaMA (via Ollama), FastAPI &amp; Streamlit


## Features
- FastAPI backend
- Streamlit frontend
- Local LLaMA inference using Ollama

## Setup Instructions
1. Clone the repository
2. Install dependencies:
   pip install -r requirements.txt
3. Start backend:
   uvicorn backend.main:app --reload
4. Start frontend:
   streamlit run frontend/app.py



ğŸ§  Text Summarizer AI

Local LLaMA Inference with FastAPI & Streamlit

A full-stack AI application that performs text summarization using a locally hosted LLaMA model via Ollama, featuring a FastAPI backend and a Streamlit frontend. This project runs entirely offline, ensuring data privacy, low latency, and zero API costs.

ğŸ“Œ Project Overview
ğŸ¯ Goal

Build a modern, modular AI application that:

Runs LLMs locally without cloud dependencies

Provides a clean API layer for inference

Offers a simple, interactive web UI for users

Follows real-world engineering best practices

ğŸš€ Features

âš¡ FastAPI backend for high-performance API handling

ğŸ¨ Streamlit frontend for real-time user interaction

ğŸ§  Local LLaMA inference using Ollama

ğŸ”’ Privacy-first (no external API calls)

ğŸ’¸ Cost-efficient (no usage fees)

ğŸ§© Modular architecture (easy to extend)

ğŸ› ï¸ Technology Stack
Component	Technology
LLM Runtime	Ollama (LLaMA)
Backend API	FastAPI
Frontend UI	Streamlit
Language	Python
Version Control	Git & GitHub
ğŸ“‚ Project Structure
text-summarizer-ai/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py          # FastAPI application
â”‚   â”œâ”€â”€ summarizer.py    # LLaMA / Ollama interaction logic
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py           # Streamlit UI
â”‚
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ .gitignore

âš™ï¸ Prerequisites

Before running the project, ensure you have:

Python 3.9+

Ollama installed

LLaMA model pulled locally

Git

Install Ollama

ğŸ‘‰ https://ollama.com

Pull a model (example):

ollama pull llama2


Verify Ollama is running:

ollama list

ğŸ§ª Setup Instructions
1ï¸âƒ£ Clone the repository
git clone <your-repo-url>
cd text-summarizer-ai

2ï¸âƒ£ Create & activate a virtual environment (recommended)
python -m venv venv


macOS / Linux

source venv/bin/activate


Windows

venv\Scripts\activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Start the FastAPI backend
uvicorn backend.main:app --reload


Backend will run at:

http://127.0.0.1:8000


Optional API docs:

http://127.0.0.1:8000/docs

5ï¸âƒ£ Start the Streamlit frontend
streamlit run frontend/app.py


Frontend will open in your browser automatically.

ğŸ” How It Works (Architecture Flow)

User inputs text in Streamlit UI

Streamlit sends a request to FastAPI

FastAPI formats the prompt

Request is sent to local LLaMA via Ollama

Model generates summary

Summary is returned to UI in real time

ğŸ“ˆ Example Use Cases

Summarizing long articles or documents

Private data processing without cloud exposure

Offline AI demos

LLM experimentation and prototyping

ğŸ”® Future Enhancements

Docker containerization

Authentication & rate limiting

Multiple model selection

Streaming token responses

Logging & monitoring

CI/CD pipeline

ğŸ§  Key Takeaways

Demonstrates LLM integration without cloud APIs

Showcases API design + frontend development

Highlights privacy-focused AI architecture

Easily extendable to chatbots, Q&A systems, or agents

ğŸ§‘â€ğŸ’» Author

Khaleef Haughton
Built for learning, showcasing AI + backend engineering, and real-world system design.

ğŸ“œ License

This project is open-source and available for educational and personal use.
